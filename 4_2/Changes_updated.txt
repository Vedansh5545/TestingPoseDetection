
**Detailed Explanation of Changes and Code Workflow**  

---

## Overview

Initially, a custom Graph Convolutional Network (GCN) model trained on MPI-INF-3DHP data was used to predict 3D joint positions from 2D MediaPipe keypoints. Multiple issues arose:

- **Joint-Index Mismatch**: The GCN was trained on 28 MPI-INF joints, whereas MediaPipe produces 33 landmarks in a different order. Attempts to slice or remap indices repeatedly led to incorrect bone connections.
- **Normalization & Scaling Errors**: Projecting the GCN’s raw 3D outputs back to 2D (for overlay) used min-max or naive normalization, causing skeletons to collapse into straight lines or go off-screen.
- **Visualization Bugs**: The `draw_3d_pose` function assumed the model’s output was already in normalized [0,1] range and used a skeleton edge list not aligned with MediaPipe’s output. This made 3D drawing inaccurate.

To resolve these, we overhauled large parts of the pipeline. We ultimately replaced the GCN-based overlay with a direct 2D overlay using MediaPipe’s landmarks, ensuring pixel-perfect alignment. We then added utilities to embed that “flat” 2D pose into 3D (Z=0) for debugging and bone classification.

Below is a detailed rundown of **all changes**, **why** they were necessary, **how** the final code works, and a concise description of the original model and data.

---

## Part 1: Original GCN Model and MPI-INF-3DHP Data

### 1. Model Architecture (`model.py`)

- **PoseEstimator**  
  - **Input:** Tensor of shape `(B, 28, 3)` — 28 joints, each (x,y,z=0).  
  - **PositionalEncoding:** A learned sinusoidal encoding added to the node features so that the Transformer can distinguish joint indices.  
  - **SparseGCNBlock:** Two stacked GCNConv layers (from PyTorch Geometric) with residual connections and LayerNorm + ReLU. These propagate information along skeleton edges.  
  - **AttentionRoutingTransformer:** A 2-layer Transformer encoder (4 heads, d_model=128) that refines node features globally.  
  - **Head (MLP):** Applies a Linear → ReLU → Linear to map each node’s 128-dim embedding to a final 3D coordinate (x,y,z).  

- **Edge Index Definition (`create_edge_index()` in `model.py`):**  
  ```python
  edges = [
      (0, 1), (1, 8), (8, 12),
      (1, 2), (2, 3), (3, 4),
      (1, 5), (5, 6), (6, 7),
      (8, 9), (9, 10), (10, 11),
      (8, 13), (13, 14), (14, 15),
      (0, 16), (0, 17)
  ]
  edges += [(j, i) for i, j in edges]
  return torch.tensor(edges, dtype=torch.long).t().contiguous()
  ```
  This encodes the standard MPI-INF-3DHP skeleton connectivity: head-to-neck, limbs, torso branches. The GCN conv layers use this edge index.

### 2. MPI-INF-3DHP Dataset Extraction

- **Dataset Source:** MPI-INF-3DHP contains synchronized multi-view 2D/3D annotations for 28 joints.  
- **Extraction Script (`extract_mpi_inf_3dhp.py`):**  
  - Loads each subject’s `annot.mat`, extracts `annot2` (2D) and `annot3` (3D).  
  - Reshapes them to `(frames, joints, coords)`.  
  - Filters frames by `valid_frame`.  
  - Saves each sequence as a compressed `.npz` with keys `pose2d` (shape `[F, 28, 2]`) and `pose3d` (shape `[F, 28, 3]`).  
  - Finally concatenates all subjects/sequences into `mpi_inf_combined.npz` containing arrays:  
    ```python
    pose2d = np.concatenate([all_pose2d])  # shape: (N_total, 28, 2)
    pose3d = np.concatenate([all_pose3d])  # shape: (N_total, 28, 3)
    ```
- **Normalization Stats:** In `MPIINF3DHPDataset.__init__` (in `train_pose_model_v2.py`), 2D data is zero-padded to `(28, 3)` by adding a z-coordinate of 0. Then:
  ```python
  self.pose2d_mean = pose2d.mean(axis=(0, 1))  # shape: (3,)
  self.pose2d_std  = pose2d.std(axis=(0, 1))   # shape: (3,)
  self.pose2d = (pose2d - self.pose2d_mean) / (self.pose2d_std + 1e-6)
  ```
  These stats are saved to `pose2d_mean_std.npy`, so inference code can apply identical normalization.

### 3. Training Script (`train_pose_model_v2.py`)

- **Dataset:** `MPIINF3DHPDataset("mpi_inf_combined.npz")`  
  - Returns each sample as `{ 'pose2d': Tensor(shape [28, 3]), 'pose3d': Tensor(shape [28, 3]) }`.  
- **Edge-based Bone Loss:**  
  ```python
  def bone_length_loss(preds, edges):
      loss = 0.0
      for (i, j) in edges:
          bone_lengths = torch.norm(preds[:, i] - preds[:, j], dim=-1)
          loss += torch.mean((bone_lengths - 1.0)**2)
      return loss
  ```
  Encourages predicted bone lengths to remain close to 1 (unit length after normalization).
- **Training Loop:**  
  - Optimizer: `Adam(lr=1e-3, weight_decay=1e-4)`  
  - LR Scheduler: `ReduceLROnPlateau` on validation MPJPE  
  - Loss = `MPJPE(outputs, targets) + 0.01 × bone_length_loss(outputs, bone_edges)`  
  - Best model saved to `best_model_weights.pth`; final to `final_model_weights.pth`.

---

## Part 2: Why the GCN Pipeline Failed for Accurate Overlay

1. **Joint-Index & Skeleton Mismatch**  
   - MPI-INF uses 28 joints in a particular order, but MediaPipe outputs 33 landmarks (head, shoulders, elbows, wrists, hips, knees, ankles, heels, feet, etc.) in an entirely different index mapping.  
   - Attempting to slice MediaPipe’s 33 into the 28 MPI indices or remap them consistently proved brittle. As a result, edges in the GCN’s `edge_index` did not correspond to the correct anatomical connections in the 2D image.  
   - Even after manually rearranging, small indexing mistakes made limb bones cross incorrectly or the skeleton appear collapsed.

2. **3D→2D Projection Distortion**  
   - The original `draw_3d_pose` in `visualize.py` assumed its input `joints_3d` was already in normalized [0, 1] range. It performed:
     ```python
     joints_2d = joints_3d[:, :2] * np.array([W, H])
     ```
     But GCN’s 3D output was in MPi-INF’s raw world units (millimeters, possibly centered arbitrarily). A naïve min-max normalization (clipping to [-2.5, 2.5]) caused large fluctuations.  
   - Skeletons often collapsed into nearly straight lines or extended off the screen.

3. **Visualization Mismatch**  
   - `draw_3d_pose` used a fixed `SKELETON_EDGES` list (28-joint MPI ordering) that did not align with MediaPipe’s 33 landmarks. When we tried to feed transformed MediaPipe points through it, many bones were missing or misconnected.  
   - We tried to “force” the GCN output into that function, but the underlying assumption (correct joint order + range) was never satisfied.

4. **Over‐Engineering & Debug Complexity**  
   - Combining: MediaPipe→custom GCN→custom 3D alignment→custom 2D overlay introduced multiple error sources. Debugging time ballooned, and final overlays still looked inaccurate.  
   - Pixel accuracy in 2D was nonnegotiable; a simpler direct overlay was guaranteed to be correct.

**Conclusion:** We replaced the GCN‐based overlay entirely and built a direct 2D overlay using MediaPipe’s 2D landmarks, eliminating all remapping/normalization/projection issues.

---

## Part 3: All New Code Changes & Additions

Below is a chronological list of _all_ new changes made in this project, in the order they were introduced during our discussion:

---

### 1. **Direct 2D Overlay with MediaPipe**  
- **Created** `visualize.py` for a pixel-perfect 2D overlay (no GCN, no projection).  
  - **Added** `POSE_CONNECTIONS` (MediaPipe’s built-in 33‐point skeleton connectivity).  
  - **draw_2d_pose()**:  
    - Converts each normalized `(lm.x, lm.y)` → pixel `(x_px, y_py)` by multiplying by `frame.shape[:2]`.  
    - Draws **green lines** between each connected pair `(i, j)` in `POSE_CONNECTIONS`.  
    - Draws **red circles** at each joint with index labels.  

- **Created** `predict_image.py` to run that overlay:  
  1. Initialize MediaPipe Pose (static image mode).  
  2. Read & resize input: `input.jpeg` → 480×480.  
  3. Run `pose.process(frame_rgb)` → `results.pose_landmarks`.  
  4. Call `draw_2d_pose(frame.copy(), landmarks)` → draw bones + joints.  
  5. Save as `output.jpg` for pixel‐perfect overlay.  

  *Reason:* Guarantee that the 2D skeleton aligns exactly with the image; there is no remapping or projection error.

---

### 2. **“Flat” 2D→3D Embedding (Z=0 Plane)**  
These scripts embed a 2D pose into a 3D coordinate system by setting all Z = 0, then plot in Matplotlib’s 3D.

#### a) `plot_2d_in_3d_corrected.py`  
- **extract_2d_keypoints(image_path):**  
  - Uses MediaPipe to get 33 landmarks; converts each `(lm.x, lm.y)` → `(x_px, y_px)`.  
  - Keeps only first 28 joint points (for MPI compatibility).  
  - Returns `joints_2d (28×2)` + `(w, h)`.

- **plot_flat_skeleton_3d(joints_2d, (w, h)):**  
  - **Flip Y:** In image space, `y=0` is top; in Cartesian plot, `y` grows upward.  
    ```python
    x_plot = x_px
    y_plot = h - y_px
    Z = 0.0
    ```  
  - **Build `edges`** using MediaPipe’s `mp_pose.POSE_CONNECTIONS`, filtered to only `(i, j)` where both `< 28`.  
  - **Plot:**  
    - Red dots at `(x_plot, y_plot, 0)` for each joint.  
    - Green lines between joints per `edges`.  
    - Labels joint indices in blue.  
    - Sets `ax.view_init(elev=90, azim=-90)` to look straight down on Z=0.  
    - Equalizes X–Y axis ranges so pose isn’t stretched.

  *Reason for Y flip:* Ensures that a joint near the top of the image (small `y_px`) appears near the top of the 3D plot (large `y_plot`).

#### b) `plot_2d_in_3d_with_bone_labels_and_legend.py`  
- **extract_2d_keypoints()** same as above, but also computes:
  ```python
  left_hip_y  = raw_pts[23][1]
  right_hip_y = raw_pts[24][1]
  hips_y = (left_hip_y + right_hip_y) / 2.0
  ```
- **plot_flat_skeleton_with_labels_and_legend(joints_2d, hips_y, (w, h)):**  
  - Flip Y same as before → `pts3d` shape `(28, 3)`.  
  - Build `edges` from `POSE_CONNECTIONS` filtered to `< 28`.  
  - **Classify each bone** (edge `(i, j)`):  
    - If both `joints_2d[i][1] < hips_y` and `joints_2d[j][1] < hips_y`: **upper** → red.  
    - If both `> hips_y`: **lower** → blue.  
    - Else: **torso/mixed** → green.  
  - Plot:  
    - Red joint dots.  
    - Each bone line in its classification color.  
    - Label the midpoint of each bone with “upper” / “lower” / “torso” in matching color.  
    - Joint indices annotated in black.  
    - **Legend** added with:  
      ```python
      legend_elements = [
          Line2D([0],[0], color='r', lw=3, label='Upper-body bone'),
          Line2D([0],[0], color='b', lw=3, label='Lower-body bone'),
          Line2D([0],[0], color='g', lw=3, label='Torso/mixed bone')
      ]
      ax.leg
